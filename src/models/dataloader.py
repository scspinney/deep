# -*- coding: utf-8 -*-
"""dl1_enigma_drug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o7ohIUvnkFyBrRMyNWm1TGrLhXvw1wpj
"""

from nilearn.image import index_img, smooth_img
from nilearn.masking import apply_mask
from nibabel.nifti1 import Nifti1Image
import os
import torch
from torch import Tensor, nn
from torch.utils.data import DataLoader, Subset, Dataset, TensorDataset, random_split
from torch.utils.data.sampler import WeightedRandomSampler, SubsetRandomSampler
from torchvision import transforms
import pytorch_lightning as pl
import numpy as np
import glob
import pandas as pd
import math
from functools import partial
from argparse import ArgumentParser
from pytorch_lightning.loggers import WandbLogger
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
import wandb
import torchio as tio

from nilearn.image import crop_img, resample_to_img

import warnings
from typing import (
    Callable,
    ClassVar,
    Dict,
    Iterable,
    List,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    Type,
    Union,
    Any
)


class Subsample(object):
    """subsample the 3D image of a timestep.

    Args:
    """

    def __init__(self, subsample_rate_x, subsample_rate_y, subsample_rate_z):
        self.rate_x = subsample_rate_x
        self.rate_y = subsample_rate_y
        self.rate_z = subsample_rate_z

    def __call__(self, sample):
        dim_x = sample.shape[0]
        dim_y = sample.shape[1]
        dim_z = sample.shape[2]

        indexes_x = range(0, dim_x, self.rate_x)
        indexes_y = range(0, dim_y, self.rate_y)
        indexes_z = range(0, dim_z, self.rate_z)

        sample = sample[indexes_x, :, :]
        sample = sample[:, indexes_y, :]
        sample = sample[:, :, indexes_z]

        return sample


"""Data processing:"""


class MRIDataModuleIO(pl.LightningDataModule):
    def __init__(self, data_dir: str, labels: List[int], format: str, batch_size: int, augment: List[str],
                 mask: str = '', file_paths: List[str] = None, num_workers: int = 1, sampler: bool = True):
        super().__init__()
        self.data_dir = data_dir
        self.labels = torch.tensor(labels)
        self.format = format
        self.n = len(labels)
        self.n_train = int(.9 * self.n)
        self.n_test = self.n - self.n_train
        self.mask = mask
        self.batch_size = batch_size
        self.augment = augment
        self.file_paths = file_paths
        self.num_workers = num_workers

        shuffled_ind = np.random.choice(range(self.n), len(range(self.n)), replace=False)

        self.train_labels = self.labels[shuffled_ind[:self.n_train]]
        self.test_labels = self.labels[shuffled_ind[self.n_train:]]
        self.train_paths = self.file_paths[shuffled_ind[:self.n_train]]
        self.test_paths = self.file_paths[shuffled_ind[self.n_train:]]
        self.sampler = sampler

        # check test distribution
        class_sample_count_test = torch.tensor(
            [(self.test_labels == int(t)).sum() for t in torch.unique(self.labels, sorted=True)])

        class_sample_count_train = torch.tensor(
            [(self.train_labels == int(t)).sum() for t in torch.unique(self.labels, sorted=True)])

        print(f"Class distribution in test set: {class_sample_count_test}")
        print(f"Class distribution in train set: {class_sample_count_train}")

    def get_max_shape(self, subjects):

        preprocess = tio.Compose([
            tio.EnsureShapeMultiple(2)
        ])

        dataset = tio.SubjectsDataset(subjects, transform=preprocess)
        shapes = np.array([s.spatial_shape for s in dataset])
        self.max_shape = shapes.max(axis=0)
        return self.max_shape

    def prepare_data(self):
        image_training_paths = self.train_paths
        label_training = self.train_labels
        image_test_paths = self.test_paths
        label_test = self.test_labels

        self.subjects = []
        for image_path, label in zip(image_training_paths, label_training):
            # 'image' and 'label' are arbitrary names for the images
            subject = tio.Subject(
                image=tio.ScalarImage(image_path),
                label=label
            )
            self.subjects.append(subject)

        self.test_subjects = []
        for image_path, label in zip(image_test_paths, label_test):
            subject = tio.Subject(image=tio.ScalarImage(image_path),
                                  label=label)
            self.test_subjects.append(subject)

    def get_preprocessing_transform(self):
        preprocess = tio.Compose([
            tio.CropOrPad(self.get_max_shape(self.subjects + self.test_subjects)),
            tio.EnsureShapeMultiple(2),
            tio.RescaleIntensity((-1, 1)),
        ])
        return preprocess

    def get_augmentation_transform(self):

        if self.augment:
            augment = []
            for a in self.augment:
                if a == 'affine':
                    augment.append(tio.RandomAffine())
                elif a == 'noise':
                    augment.append(tio.RandomNoise(p=0.3))

                elif a == 'motion':
                    augment.append(tio.RandomMotion(p=0.2))

            augment = tio.Compose(augment)
            return augment
        else:
            return None

    def setup(self, stage=None):

        indices = range(self.n_train)  # np.random.choice(range(self.n_train), range(self.n_train), replace=False)
        split = int(np.floor(.2 * self.n_train))
        train_indices, val_indices = indices[split:], indices[:split]
        # Creating PT data samplers and loaders:
        if self.sampler is not True:
            self.train_sampler = SubsetRandomSampler(train_indices)
            self.val_sampler = SubsetRandomSampler(val_indices)
        else:

            self.train_sampler, self.weight = class_imbalance_sampler(self.train_labels[train_indices])

        train_subjects = [self.subjects[i] for i in train_indices]
        val_subjects = [self.subjects[i] for i in val_indices]

        self.preprocess = self.get_preprocessing_transform()
        augment = self.get_augmentation_transform()
        if augment is not None:
            self.transform = tio.Compose([self.preprocess, augment])
        else:
            self.transform = self.preprocess

        self.train_set = tio.SubjectsDataset(train_subjects, transform=self.transform)
        self.val_set = tio.SubjectsDataset(val_subjects, transform=self.preprocess)
        self.test_set = tio.SubjectsDataset(self.test_subjects, transform=self.preprocess)

    def train_dataloader(self):
        return DataLoader(self.train_set, self.batch_size, sampler=self.train_sampler, num_workers=self.num_workers,
                          drop_last=True)

    def val_dataloader(self):
        return DataLoader(self.val_set, self.batch_size, num_workers=self.num_workers, drop_last=True)

    def test_dataloader(self):
        return DataLoader(self.test_set, self.batch_size)


def get_mri_data_beta(N, data_dir, cropped=False, test=False):
    name = f"data_split_c.csv"
    df = pd.read_csv(os.path.join(data_dir, name))
    labels = []

    dfg = df.groupby("class")
    data = []
    for name, subdata in dfg:
        print(f"Group: {name}")
        # shuffle
        K = subdata.shape[0]
        shuffled_ind = np.random.choice(range(K), len(range(K)), replace=False)
        # subsample
        shuffled_ind = shuffled_ind[:N]
        data.extend(subdata["filename"].values[shuffled_ind])
        labels.extend(subdata["class"].values[shuffled_ind])

    # shuffle
    data = np.array(data).reshape(-1)
    labels = np.array(labels).reshape(-1)
    ind = np.random.choice(len(labels), len(labels), replace=False)

    labels = labels[ind]
    data = data[ind]

    assert data.shape[0] == labels.shape[0]

    return data, labels